{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Neural Network (MLP)\n",
    "This Notebooks shows the implementation of a Neural Network fully vectorized including backpropagation and inference"
   ],
   "id": "4be8af1e414e985f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T20:45:30.250691Z",
     "start_time": "2025-10-22T20:45:29.347990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ],
   "id": "e64dbae591c7199e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Activation Functions\n",
    "Non-linear activation functions allow neural networks to learn complex patterns in data.\n",
    "\n",
    "## Linear\n",
    "No transformation applied. Used for regression output layers.\n",
    "\n",
    "$$f(x) = x$$\n",
    "\n",
    "## ReLU\n",
    "Most popular for hidden layers. Computationally efficient and helps avoid vanishing gradients.\n",
    "\n",
    "$$f(x) = \\max(0, x)$$\n",
    "\n",
    "## Tanh\n",
    "Squashes values to (-1, 1). Zero-centered, often better than sigmoid for hidden layers.\n",
    "\n",
    "$$f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "## Sigmoid\n",
    "Squashes values to (0, 1). Used for binary classification output layers.\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "## Softmax\n",
    "Used for multi-class classification output layers. Converts logits to probabilities.\n",
    "$$f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$\n",
    "\n",
    "**Note:** There are many more activation functions available (e.g., Leaky ReLU, ELU, GELU, Swish, etc.), each with their own characteristics and use cases.\n",
    "\n"
   ],
   "id": "f41b09e8ffa82dd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T20:45:30.268759Z",
     "start_time": "2025-10-22T20:45:30.263355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Linear doesn't need an implementation (f(x) = x)\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract max along axis for numerical stability\n",
    "    x_shifted = x - np.max(x, axis=axis, keepdims=True)\n",
    "    exps = np.exp(x_shifted)\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)"
   ],
   "id": "afc62d35f625db61",
   "outputs": [],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
