{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Neural Network (MLP)\n",
    "This Notebooks shows the implementation of a Neural Network fully vectorized including backpropagation and inference"
   ],
   "id": "4be8af1e414e985f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T20:45:30.250691Z",
     "start_time": "2025-10-22T20:45:29.347990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ],
   "id": "e64dbae591c7199e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Activation Functions\n",
    "Non-linear activation functions allow neural networks to learn complex patterns in data.\n",
    "\n",
    "## Linear\n",
    "No transformation applied. Used for regression output layers.\n",
    "\n",
    "$$f(x) = x$$\n",
    "\n",
    "## ReLU\n",
    "Most popular for hidden layers. Computationally efficient and helps avoid vanishing gradients.\n",
    "\n",
    "$$f(x) = \\max(0, x)$$\n",
    "\n",
    "## Tanh\n",
    "Squashes values to (-1, 1). Zero-centered, often better than sigmoid for hidden layers.\n",
    "\n",
    "$$f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "## Sigmoid\n",
    "Squashes values to (0, 1). Used for binary classification output layers.\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "## Softmax\n",
    "Used for multi-class classification output layers. Converts logits to probabilities.\n",
    "$$f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$\n",
    "\n",
    "**Note:** There are many more activation functions available (e.g., Leaky ReLU, ELU, GELU, Swish, etc.), each with their own characteristics and use cases.\n",
    "\n"
   ],
   "id": "f41b09e8ffa82dd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T20:45:30.268759Z",
     "start_time": "2025-10-22T20:45:30.263355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Linear doesn't need an implementation (f(x) = x)\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract max along axis for numerical stability\n",
    "    x_shifted = x - np.max(x, axis=axis, keepdims=True)\n",
    "    exps = np.exp(x_shifted)\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)"
   ],
   "id": "afc62d35f625db61",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Loss Functions\n",
    "\n",
    "## Mean Squared Error (MSE)\n",
    "\n",
    "Used for regression tasks, it measures the average squared difference between actual and predicted values:\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 $$\n",
    "\n",
    "## Cross Entropy Loss\n",
    "\n",
    "Used for classification tasks to compare predicted probabilities with true labels.\n",
    "\n",
    "### Binary Cross Entropy\n",
    "\n",
    "For two-class problems:\n",
    "$$ L_{BCE} = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] $$\n",
    "$y_i$ = true label (0 or 1), $\\hat{y}_i$ = predicted probability.\n",
    "\n",
    "### Categorical Cross Entropy\n",
    "\n",
    "For multi-class problems where each sample belongs to one class:\n",
    "$$ L_{CCE} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c}) $$\n",
    "\n",
    "* $C$ = number of classes\n",
    "* $y_{i,c}$ = 1 if correct class, else 0\n",
    "* $\\hat{y}_{i,c}$ = predicted probability for class $c$\n"
   ],
   "id": "75a854e192a5ecd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def mse(y, y_hat):\n",
    "    return np.mean((y-y_hat) ** 2)\n",
    "\n",
    "def binary_cross_entropy(y, y_hat):\n",
    "    return - np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "def categorical_cross_entropy(y, y_hat):\n",
    "    # Multiply y * log(y_hat) keeps only the correct class log-probabilities\n",
    "    # axis=1 sums across classes for each sample, then np.mean averages over the batch\n",
    "    return -np.mean(np.suxm(y * np.log(y_hat)), axis=1)"
   ],
   "id": "8fba245d3b5587cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
